\section{Methodology}
\label{framework:methodology}

The implementation of the Named Entity Disambiguation Framework with all its underlying microservices represent the tool which is used to study and answer the first two research questions. The majority of the implementation patterns and algorithms are based on theoretical background and reviewed literature on the respective field. In absence of open source code, the \ac service has been implemented as a reproduction of the work conducted by Chabchoub et al.~\cite{39}. However, without actual contribution of human annotators by participating in a user study, the answers to the research questions would not have been resolved. The first experimental user study has been conducted in order to get detailed insights on how well the information processed and presented to human annotators would contribute to generating annotation data. The reason that two user studies were conducted during this research work was to measure the engagement and playfulness of the game when compared to a standard, non-gamified interface for named entity disambiguation. Data from both experiments have been used to analyze and compare the different characteristics in order to draw on conclusions that we initially set to achieve. Regardless, this methodology section represents all the necessary information needed to successfully conduct the first experiment and describe the techniques and methodologies used to analyze the results.

%Datasets 
In order to conduct the annotation experiment by using the designed AnnotateMe Interface, an annotation data gathering process was conducted beforehand. In absence of linguistic and expert annotators to construct a gold standard which would have been used for assessing the quality of annotations, we decided to use already existing datasets, namely, Spotlight and KORE50 datasets \cite{datasets-ex1}. The first dataset contains documents from news articles whereas the second one contains short articles extracted from various domains. KORE50 is known by the NLP experts as a dataset characterized with a highly ambiguous nature. Table \ref{tab:experimentone_dataset} summarizes all the different entity types that are recognized in each dataset used in our evaluation experiment. As observed from the table, both datasets are composed mostly of entities in the three main categories: Organization, Location and People. The table was originally reported by Steinmetz et al. \cite{datasets-ex1}, and during the time of their writing, some of the entity mentions recognized in the dataset did not have a \ac{kb} representative. However, it has been mentioned that the information in \ac{lod} is continuously increased by researchers contributing with new datasets and extending existing ones, the ratio of mentions and entities from both datasets is likely to be more equalized now\footnote{The number of entity mentions recognized in the text having a corresponding \ac{kb} representative is increased.}. However, the analysis and results presented in the next section are up-to-date and take into consideration the latest versions of KROE50 and Spotlight datasets. 

\newpage
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Distribution of entity types for Spotlight and KORE50 \cite{datasets-ex1}}
    \begin{tabular}{|l|rr|rr|}
    \toprule
    \multicolumn{1}{|c|}{\multirow{Class}} & \multicolumn{2}{c|}{Spotlight } & \multicolumn{2}{c|}{KORE50} \\
\cmidrule{2-5}          & \multicolumn{1}{l|}{Entities} & \multicolumn{1}{l|}{Mentions} & \multicolumn{1}{l|}{Entities } & \multicolumn{1}{l|}{Mentions} \\
    \midrule
    Total & 249   & 331   & 130   & 144 \\
    Agent & 1.40\% & 2.70\% & 66.90\% & 70.80\% \\
    -Organization & <1\%  & <1\%  & 19.40\% & 5.30\% \\
    --Company & <1\%  & <1\%  & 9.20\% & 9.70\% \\
    --Sports Team & -     & -     & 7.70\% & 6.90\% \\
    ---Socer Club & -     & -     & 7.70\% & 7.90\% \\
    -Person & 2.00\% & 2.40\% & 48.50\% & 51.40\% \\
    --Artist  & -     & -     & 17.70\% & 18.80\% \\
    ---MusicalArtist & -     & -     & 17.70\% & 18.80\% \\
    --Athlete & -     & -     & 6.90\% & 8.30\% \\
    ---SoccerPlayer & -     & -     & 5.40\% & 6.30\% \\
    --OfficeHolder & <1\%  & <1\%  & 4.60\% & 4.20\% \\
    Disease & 1.60\% & 1.20\% & -     & - \\
    EthnicGroup & 1.20\% & 1.80\% & -     & - \\
    Event & 1.20\% & <1\%  & -     & - \\
    Place & 10.40\% & 10\%  & 10.80\% & 10.40\% \\
    -Architectural Structure & 2.00\% & 1.50\% & 3.10\% & 2.80\% \\
    --Infrastructure & 1.60\% & 1.20\% & <1\%  & <1\% \\
    -PopulatedPlace & 7.20\% & 7.60\% & 5.40\% & 5.50\% \\
    --Country & 3.60\% & 3.30\% & -     & - \\
    --Region & <1\%  & <1\%  & -     & - \\
    --Settlement & 2.40\% & 3.30\% & 3.80\% & 3.50\% \\
    ---City & 1.60\% & 2.10\% & 2.30\% & 2.10\% \\
    Work  & <1\%  & <1\%  & 6.20\% & 6.30\% \\
    -MusicalWork & <1\%  & <1\%  & 3.10\% & 3.50\% \\
    --Album & <1\%  & <1\%  & 3.10\% & 3.50\% \\
    \bottomrule
    \end{tabular}%
  \label{tab:experimentone_dataset}%
\end{table}%

%recruiting participants
The participants who took part in the annotation experiment were invited through emails and social media. On the invitation message sent to all participants, a short and abstract description of the idea behind the experiment was provided. It is important to note that payment incentives and any other type of incentive which would degrade the voluntary participation of the user were avoided, thus assuring a complete voluntary participation without any beneficial intentions. As a result, 30 participants showed up and successfully completed the experiment.

The experiment was conducted in closed group rooms at the university campus in order to make sure that the participant was not being distracted while performing the experiment. Before starting to perform the actual annotations, the participants were presented with a consent form which explained the nature, purpose and intentions of the experiment. Participants were also fully aware that the participation was anonymous and voluntary and withdrawal from participation was possible at any time. The consent form for the first experiment is provided in Appendix \ref{appendix1:annotatme}.

On average, the duration of a single experiment session was about 25 minutes long. We also asked the participants to fill in a pre-questionnaire to gather demographic information about them as well as questions that assessed the level of expertise of each participant in the field of semantic web and natural language processing. In order to avoid potential biases on the results, we made sure that participants had moderate to native skills in English. We report on the demographic data of participants in the next section whereas the questions provided in the pre-questionnaire are presented in Appendix \ref{appendix1:annotatme}.

The non-expert nature of participants and the time constraints on the duration of an experiment session for each participant was the main reason for recording an instructional video to be presented to each participant before starting the actual annotation task. This video was essentially a replacement for an introductionary interface that is usually implemented for such experiments. During the 4 minute demonstrative video, we demonstrated to the participants how the interface is used and the purpose of each elements towards the general idea of annotating. After having read the consent form, filled in the pre-questionnaire and watched the demo video, the participants proceeded in doing the annotations for 15 minutes. However, the experimenter did not provide upfront information on how many annotations had to be performed. After the estimated 15 minutes elapsed, the participants were instructed to either finish the experiment or continue do more annotations This was a technique we used for measuring the level of enjoyment that participants had towards the interface. After the participant free-willingly\footnote{A free-will annotation is an annotation performed by a participant after being aware that the experiment was completed. This is a metric to measure the attractiveness of and engagement with the interface as perceived by the participant.} decided to finish the experiment, he or she was asked to fill in a post-questionnaire. The post-questionnaire contained questions with the purpose of assessing the quality, usability and engagement level of the interface. We report on the results of the post-questionnaire in the next section. The questions provided in the post-questionnaire can be found in Appendix \ref{appendix1:annotatme}.

% Assessment methodologies, f-measures, annova analysis, accuracy calcuations (for NER and Automatic annotator)
For assessing the performance of the framework in terms of entity recognition, user annotation quality and agreement level, we used metrics such as precision, recall, f-score and one way ANOVA analysis of variance. These assessment methodologies are commonly used in entity linking systems \cite{12}. Furthermore, to be able to assess the performance of our approach with state-of-the-art frameworks we have used a generic benchmark called GERBIL  originally developed by Usbeck et al. \cite{40}. GERBIL is a comparison tool for easily discovering the strengths and weaknesses of your implementation with respect to the state of the art in named entity disambiguation. The tool is open source and is an extensible framework that currently supports 9 different annotations on 11 different datastes within 6 different data types (recognition, disambiguation, linking etc). We report on the results of our framework with respect to the state-of-the-art annotators offered by GERBIL for Spotlight and KORE50 datasets. 

% Justify why we have used the following methodology. Why is it best fit for our study and answering our research questions

% Likely sources of BIAS
Concerning limitations and potential sources of bias, the methodology we used for our research study can be considered partially immune. For the sake of reproducability, we need to note that the participants who were invited through emails and social media were within the scope of the university campus. Consequently, one might argue that they might have been biased to participate in the experiment. Since this was seen as the only way of recruiting the participants in the pressure of time and space, we account this as a potential source of bias and address as a limitation of the recruitment methodology. However, we assure consistency with regard to participant recruitment since the same approach was used to recruit participants for the second experiment as well. Therefore, in case the results indicate increased engagement and motivation by using the game compared to the plain interface, we are able to claim that the improvements are acceptable since we remain consistent in the methodology. 

%Hypothesis 
\paragraph{Hypothesis}
The aim of the first experiment is to find out whether the implemented framework supports qualitative annotations by non-expert annotators. Therefore, we hypothesize that:

\begin{itemize}
    \item H1.1: By implementing the complete entity disambiguation pipeline as a framework, non-expert users will be able to perform annotations with a quality compared to expert annotators!
    \item H1.2: Short contextual clues are preferred towards complete sentences or paragraphs and as such provide sufficient information to make correct disambiguations! 
\end{itemize}


