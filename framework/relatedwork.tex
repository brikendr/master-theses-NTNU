\section{Related Work}
\label{framework:relatedwork}

% MN: Do you have related work to the process of using games with a purpose or gamification for disambiguation? Perhaps you should have a paragraph about it. If there is no work directly for gamification for NED, try something close enough, that fits, or, that you can use inspiration from.

Named Entity Disambiguation is not a new problem and previous research have tried to improve it using various approaches ranging from supervised automatic algorithms that rely on classifiers, machine learning algorithms to manual approaches that rely on human input for manually performing the disambiguation step. Therefore, in this section we try to summarize previous research studies with special emphasis in human-related approaches as well as automatic techniques since our ultimate goal is contributing to the improvements of supervised approaches for \ac{ned} until an acceptable level of accuracy is reached. 

\subsection{Automatic Approaches for Entity Disambiguation}
\label{framework:relatedword_automatic}

% MN: Prefix citations and references with a tilde sign, this way there will be fixed width spacing retained between the last word and the citation/reference.

One of the earliest attempts to model an Entity Disambiguation process (recognition and linking of surface forms) was performed by \cite{8}. They modeled a system called Wikify! which, given an input document, was able to identify the important concepts in text and link them to the corresponding Wikipedia pages. They have utilized a sense inventory (Wordnet\footnote{Wordnet Lexical Database \url{https://wordnet.princeton.edu/}}) and a link probability algorithm for disambiguating the ambiguous surface forms and linking them with the correct wiki page. Wikify's approach for disambiguating a surface form is by extracting features from the phrase and its surrounding context and compares it with training examples extracted from the entire Wikipedia. Linden~\cite{20} links named entities with a KB by unifying Wikipedia and Wordnet. They use four Wikipedia sources for collecting information about the surface forms, namely: entity pages, redirect pages, disambiguation pages and hyperlinks in Wikipedia articles. This information is then used to generate candidate list for each entity mention. Linden does the disambiguation process by combining the following measures: link probability, semantic associative, semantic similarity and global coherence~\cite{20}. However, the drawback of these approach is that they require massive pre-processing effort (parsing the entire Wikipedia)~\cite{9}.

Other research studies~\cite{6, dbpedia}, have used a controlled vocabulary for identifying entity mentions in the document. For the disambiguation process, Turian~\cite{6} used two sets of features, namely \textit{link probability} of an entity mention and \textit{commonness} of the candidates. 
%According to \cite{6}, \textit{link probability} represents the number of times a surface form occurs as an entity mention in a knowledge base (KB), divided by its total occurrences. \textit{Commonness} of a candidate c for a surface form s, is defined as the fraction between the times that s refers to c and the total number of times that s refers as a mention to any other entity. 
In order to guarantee the accuracy of entity linking, the candidates selected during the disambiguation process have to be strongly related with the target entity (that means higher values of commonness and link probability).~\cite{6}

The EDL Framework~\cite{19} is a similar approach compared to Wikify! where the disambiguation process is done using a combination of search engine results and knowledge base repository mining. Their framework consists of three steps: querying a \ac{kb} for identifying potential candidates for the entities extracted in text, querying search engines for the same purpose and finally comparing the results from these two steps and output the best matching candidate for a particular entity. This is an unsupervised approach that relies only on features for disambiguating candidate entities.~\cite{19}

Unlike previous approaches, Hoffart et al.~\cite{21} argue that the key for further improvements in the named entity disambiguation process it to jointly consider multiple mentions when ranking the candidates. They argue that, when disambiguating an entity, the framework should consider also other named entities in a collective manner in order to select the correct candidate describing the entity~\cite{21}.

Alchemy API is a framework for semantic annotation developed by Watson LAB\footnote{IBM Watson Alchemy API \url{https://www.ibm.com/watson/alchemy-api.html}}. Alchemy API analyzes WEB or textual content by using built-in \ac{nlp} techniques, machine learning algorithms and other complex linguistic, statistical and neural network algorithms. The Alchemy API framework provides functionalities similar to our framework except that no human validation is used here. We used Alchemy API for generating document topic keywords as part of our context-clue generation modules. 

 DBPedia Spotlight~\cite{dbpedia} is a system for automatically annotating text documents with DBpedia\footnote{DBpedia Knowlege Base \url{http://wiki.dbpedia.org/}} URLs. The goal of the service is to provide comprehensive and flexible solutions for entity annotations by offering a cross-domain vocabulary that can describe entities with diverse nature. Similar to~\cite{6}, they depend on a controlled vocabulary for recognizing entity mentions in text. In particular, they use the LingPipe Exact Dictionary-Based Chunker which is based on Hidden Markov Models~\cite{dbpedia}. Regarding the candidate selection process, they rely on their own localization dataset for determining candidate disambiguation for each entity mention. However this step does not decide on the correct candidate, it only filters out irrelevant options. The disambiguation step consists of a supervised approach using a vector representation of different context features around the surface form. They have used Vector Space Model (VSM) for modeling each DBpeida candidate as a multi-dimentional space of words represented as a vector. DBpedia Spotlight\footnote{Dbpedia Spotlight Demo \url{http://demo.dbpedia-spotlight.org/}} provides an open source and free to use Web Service API that allows third-party applications to run queries and retrieve annotations with links pointing to DBpedia concepts. We use Dbpedia Spotlight as our utilized automatic annotator for generating entity candidates identified in textual content.
 
Collective disambiguation was also used by Chabchoub et al.~\cite{39}. They utilize an open source NER system in combination with an open source automatic annotator for recognizing entities in text. Similar to our named entity recognition module, they develop matching and filtering algorithms for improving the recognition process in terms of precision and recall. Candidates for each entity mention are generated by querying DBpedia Spotlight. For ambiguous entities, where more than one candidate is retrieved, the disambiguation is done by taking into account the other entity mentions that have been already disambiguated in the text.~\cite{39}

\subsection{Human-Centered Approaches for Entity Disambiguation}
\label{framework:relatedword_automatic}
Automatic techniques for \ac{ned}, just like any other classification or learning algorithms, have not reached the level at which they can simulate the way humans think and make links between concepts and ideas~\cite{sanderson1994,30}. However, these techniques are being improved continuously by providing training data that the algorithms can learn from and thus getting closer to the ultimate goal where the human knowledge can be reproduced and taken advantage of. This is a strong justification why many research studies (referring to our study as well) are continuously trying to leverage human input until the automatic approaches are considered mature. 

Asking human annotators for validating the links generated by automatic approaches has been the focus of many research studies. ELIANTO supports human labelling of semi-structured documents by asking users to annotate entity mentions and then ranking those entities based on the perceived relevance or salience~\cite{2}. Khan et al.~\cite{5} conducted a user study where they asked participants to re-validate the system's accuracy by tagging concepts identified by the semantic annotator (Alchemy API) with their corresponding meaning. Milne et al.~\cite{9} used crowdsourcing for validating the linking accuracy of their system that used machine learning for generating candidates. Van Veen et al.~\cite{31} implemented a system for named entity linking for dutch historical documents. They used machine learning and rule-based techniques for the linking process, whereas for the validation process they asked library employees to make correction and also add missing links. All of these systems rely on users' voluntary incentive for contributing annotations. However, the voluntary incentive of participants cannot be taken for granted and therefore, participation of users in such systems in a long-time period is questionable.

Loomp OCA \cite{15,16} is a system developed for preforming annotations by non-experts annotators. However, they experience UX problems where users struggle on mitigating the complexity of the system. The work carried out by Snow et al. \cite{32} explored the use of Amazon Mechanical Turk to determine whether non-expert annotators can provide reliable annotations. They designed five different \ac{nlp} tasks, among them \ac{ned} and \ac{wsd}, and for each of them they measure the quality of annotations by comparing them with the expert annotations. However, crowdsourcing is proved to be not an ideal solution to these type of problems \cite{41}.

\subsection{Context Representation}
\label{framework:relatedword_context}
In \ac{nlp}, defining the context of the document or the surrounding context of a word, phrase or entity is generally seen as a high complex problem. Regarding \ac{ned} and \ac{wsd}, several research studies have used the \textit{bag of words} model to measure the context similarity and consider this measure as an important feature to finalize the disambiguation decision \cite{20}. However, according to Shen et al. \cite{20}, the bag of words model fails to capture various semantic relations existing between concepts. The bag of words model is nothing more than a vector representation of the context which consists of terms occurring in the window of text and their associated weights \cite{20}. 

Other research studies go beyond extracting local contextual features to extracting context features from external sources such as Wikipedia pages. Cucerzan and Silviu \cite{24} used the information present in the entities' Wikipedia page and other articles in which the entity is explicitly mentioned. Their strategy of representing the context of an entity is based on two category of references: first being the information present on the first paragraph of the target entity page and second being the starting paragraph of other entity pages which refer back to the target entity \cite{24}. 

Unlike the others, the study conducted by Chan and Samuel \cite{29} propose a semi-supervised approach for generating context templates to tackle the \ac{wsd} problem. They have used a classification algorithm called Latent Dirilecht Alloction (LDA) which represents different topic features in a form of a vector space. All the feature vectors of the ambiguous word are recast into a network model. The disambiguation process is then done by calculating pairwise similarities between the context encoded in the templates (network) and the sentence of the ambiguous word, and taking the maximum value as the correct sense for the ambiguous word (please refer to \cite{29} for a detailed explanation). A somewhat similar approach was also used by Navigli and Roberto \cite{30}. They represent context using similar lexical features such as: tokenization, POS tagging, lemmanization, word chunking and parsing. Each target word is represented as vector of features, including the context features. The vector is used as a metric for the disambiguation step in the automatic algorithms. 

Furthermore, linguistic features are proven to have a significant affect on the disambiguation of ambiguous entities. Zhou et al. \cite{38} found that nouns are more informative than verbs by around 0.3\%. They argue that nouns contain more contextual information than verbs because named entities are more salient (important).

Another important element to keep in mind when deciding on how to represent contextual information is the size of the context window. After conducting a user experiment, Bontcheva et al. \cite{33} show that exposing only the sentence where the entity appears is not sufficient. It must be noted that the dataset they conducted the experiment with, was from tweets. Showing the whole tweet instead of the sentence resulted in better improved accuracy. It can be argued that when dealing with tweets, it makes sense to show the complete text as contextual information considering the short nature of tweets (maximum 130 characters). However, for long documents, showing the whole paragraph or one preceding and one following sentence (as suggested by Bontcheva et al. \cite{33}) might degrade user experience and overwhelm the user with a lot of information to process, a fact which is avoided by our framework.
\newpage