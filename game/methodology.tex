\section{Methodology}
\label{game:methodology}
% DESCRIBE THE METHODOLOGY USED TO ANSWER THE FOLLOWING RESEACH QUESTIONS

%---- What game mechanics can be employed in the entity resolution task so that high levels of engagement are achieved while still maintaining annotation quality?
The last proposed research question posed by this study is concerned in finding out what game mechanics and design principles are best fit for our research problem in order to positively affect intrinsic motivation and achieve high levels of engagement. We are also concerned to find out whether the implemented game contributes to qualitative annotations by non-experts so that we can harvest the potential of players with different levels of expertise in annotating, age group and academic background. To be able to answer our third research question and assess the usability and performance of our implemented game, we conducted another experimental user study with the same participants who participated in the first experiment. However, a different dataset was used for the second experiment. Choosing a different dataset was important since the participants were already familiar with the two datasets used in the first study. 

%Datasets 
%--Describe the stats of the dataset, nr of entities, types etc
MSNBC first introduced by \cite{24} is the dataset used for our second user study. It contains news-wire text from MSNBC news network. The dataset was created in 2007 and contains information and facts from that period. This fact was communicated and acknowledged by the participants on the second experiment in order to avoid reference errors such as linking the entity "USA President" with the candidate "Donald Trump" instead of "George W. Bush". The complete dataset contains 20 document in total, however, we used only 12 of them in order to reduce the total number of entities to deal with. Experimenting on a reduced version of the dataset instead of the complete version does not have any implications on our results. The reason we used a reduced version of the dataset is because our game relies on multiple judgments to disambiguate an entity (4 independent judgments to be precise). Therefore, having a small pool of entities to pick from would result in more entities being resolved during our time limited user experiment. The more entities resolved during the experiment the more confident the results of our analysis. In summary, we used the documents in the following categories from the MSNBC dataset for our second experiment: Entertainment, Health, Politics, Technology, Sports and World. The gold standard for MSNBC reported 251 named entities in total with an average of 20 named entities per document.   

%recruiting participants
%--Describe the experiment (setup, duration)
%-- we used the same participants as in the last experiment 
%-- not all showed up because of sickness or not being at the campus at the time of experiment
Similar to the first experiment, emails and social media were used as communication sources for recruiting participants for the second experiment. Initially, we aimed to recruit the same participants who took part in the first experiment, however, since some of the participants reported to be away or sick during the time of experiment, we sent invitations to others as well. As a result, 26 participants took part in the second experiment with 24 of them participating for the second time. 

The university campus was the environment where the second experiment was conducted. Participation was scheduled in different time-slots with a maximum duration of 40 minutes assigned for each round. Stretching the session duration to 40 minutes has been done for the only reason of giving temporal space for participants who wished to play longer if necessary. During the invitation process, it was made clear to the participants that the (mandatory) duration of the experiment would be approximately 20 to 25 minutes including the pre-questionnaire, gameplay and post-questionnaire. Similar to the first experiment, we used a consent form to communicate the purpose and the voluntary nature of the experiment. In order to assure consistency of data given by participants, a pre-questionnaire gathering demographic information was also used in the second experiment in addition to questions related to frequency of playing video games. We report on the demographic data of participants and their frequency of playing video games in section \ref{game:results} whereas the questions used in the pre-questionnaire are presented in Appendix \ref{appendix2:fastype}. 

% Assessment methodologies
The completion of the pre-questionnaire was followed with an immediate transition to the gameplay. In order to be consistent with the first experiment, players were asked to perform game rounds iteratively until the experimenter instructed them to stop. 15 minutes were dedicated for performing game rounds (annotations). However, the game includes an onboarding phase which introduces the player with the game and gradually reveals the complexity without overwhelming the player with too much information at once. Therefore, the time used by each participant to complete the onboarding phase is not recorded as part of the 15 minutes of total gameplay. The reason is that no annotation is performed during the onboarding phase of the game. After the 15 minutes of gameplay, participant were told to either finish playing or continue as they desire. This methodology was used in the first experiment as well and allows us to assess the participant's engagement with the game and its fun aspect. Finally, a post-questionnaire was used to assess player engagement, motivation, usability and enjoyability of the game. We report on the results of the post-questionnaire in section \ref{game:results} and list the questions used in this questionnaire in Appendix \ref{appendix2:fastype}.

Agreement level with the majority and comparison of game annotations with the gold standard are the methodologies used for determining the quality of annotations performed by players while playing Fastype. Besides the free-will annotation metric used in the first and second experiment for assessing the level of engagement of participants with each interface, we also employed Von Ahn and Debish \cite{44} assessment metrics for evaluating a GWAP. Von Ahn proposes two assessment metrics: \textit{throughput} which represents the speed at which a particular player is annotating, and \textit{average lifetime play} which represents a measure of enjoyability \cite{44}.


% ANOVA for comparison of game and interface, differences in number of observations, does not have any impact on results when working with small samples
Additionally, in order to report confident results, it was necessary to perform ANOVA analysis between the two conditions (AnnotateME and Fastype) for different factors such as: enjoyability, level of engagement and the likeliness of participants  to use each interface for an actual task outside the experiment. We perform one way ANOVA analysis for this purpose. For the sake of reproducability of results, it is important to note that the observations used to calculate ANOVA for the two conditions were not the same. The observations from the second experiment were slightly smaller than those from the first experiment. However, from a statistical point of view, small differences between conditions do not affect the final results when calculating one way ANOVA. Please note that ANOVA analysis were manually performed using Excel spreadsheets. Finally, to support our claims that the Fastype game was significantly more engaging and fun to play compared to the AnnotateMe interface, besides evaluating the interfaces using the aforementioned assessment metrics we sent a final assessment questionnaire to all participants who took part in both experiments. The questionnaire consisted of one question which asked about their preferred interface for performing the task of resolving named entities. This answers to the last questionnaire allow us to strengthen the claims that the game was significantly more attractive compared to the non-gamified interface. The content of the questionnaire is presented in Appendix \ref{appendix2:fastype}.  

% Likely sources of BIAS


%Hypothesis 
\paragraph{Hypothesis}
The second experiment was conducted in order to determine whether our proposed gamified system can intrinsically motivate users to perform a tedious and boring tasks such as named entity disambiguation. Another important aspect that needs to be emphasized is the quality of annotations. It is crucial for the design of the game to avoid having distracting elements that would intentionally or unintentionally degrade the quality of annotations. Keeping these important aspects in mind, we hypothesize that: 

\begin{itemize}
    \item H2.1: By employing game mechanics and design principles based on theoretical foundations, players will be intrinsically motivated to play the game
    \item H2.2: In case H2.1 is supported, we can further hypothesize that users who have used both interfaces to perform annotations will choose the game significantly more than the plain interface.
    \item H2.3: The quality of annotations performed by players will not be degraded even after implementing game elements that do not directly contribute to the annotation task.
\end{itemize}