% - Named Entity Disambiguation 
\section{Named Entity Disambiguation (\ac{ned})}
The Named Entity Disambiguation term refers to the process of identifying potential entity mentions in textual data and linking them with the corresponding candidate from a \ac{kb} such as Dbpedia in our case. The disambiguation part from the complete term refers to selecting an entity candidate which accurately represents the named entities' meaning and sense based on the surrounding context. The so called concept of "Wikification" as explained by Trani et al. \cite{2} is a similar approach to \ac{ned} except that in their case the link associated with the entity mention corresponds to a Wikipedia page instead of an actual \ac{kb} link. 

Furthermore, \ac{ned} can also be explained by analyzing another similar NLP problem, that is, \ac{wsd} which stands for Word Sense Disambiguation. \cite{wsd} represents the task of determining the correct meaning (sense) of a word in a given context \cite{27}. \ac{wsd} is similar to entity linking/disambiguation\footnote{Please note that linking and disambiguation will be used interchangeably throughout the text, but will refer to the same conceptual idea of resolving an entity by first disambiguating it and then linking it to the knowledge base} in the sense that both problems refer to finding the correct reference of the spotted mention in an unstructured text document \cite{27}.

Defining the correct sense of a word or entity mention means knowing how to formulate the surrounding context which gives critical information to either the human or machine annotator for making an informed decision. The aforementioned process is considered as an AI-complete problem for machines, which in analogy to \ac{np}-completeness in complexity theory is a problem whose difficulty is equivalent to solving central problems of Artificial Intelligence (\ac{ai} \cite{30}. When attacking these problems in an automatic fashion using \ac{ai} algorithms, prior knowledge is required beforehand. According to Navigli \cite{30}, given a set of words, the procedure followed by a \ac{wsd} system starts by applying techniques which make use of one or more sources of knowledge to associate the most accurate senses with words in surrounding context. In analogy, \ac{ned} and \ac{wsd} can be seen as classification tasks where the candidate words or senses are the actual classes and usually an automatic classification algorithm is applied to assign a class to each named entity or word occurrence. The association should come as a result of making a decision that is based on evidence from the surrounding context and from potential external knowledge sources such as dictionaries \cite{30}. Although the approach investigated in our research study relies on manual human annotation, improving the automatic supervised classification techniques is the ultimate goal provided with enough training data.

Automatic approaches on the other hand can be classified in three different classes: 
\begin{itemize}
    \item Unsupervised,
    \item Semi-Supervised, and
    \item Supervised
\end{itemize}

Among these three categories, supervised approaches have proven to perform best in terms of accuracy and quality of annotations \cite{29}. Unsupervised approaches usually rely on unlabeled corpora, and do not utilize manually sense-tagged data to provide a sense choice for a word in context \cite{30}. Semi-supervised approaches, just like the name implies, also rely on unlabeled corpora but in addition to that, they use various classifiers which are trained on a smaller set of trained samples. 

In contrast to the previous approaches, supervised methods use machine learning techniques to learn classifiers from labeled training sets. Furthermore, feature sets such as Part-of-Speech (POS) of neighboring words, local collocations\footnote{Collocations are also known as bigrams. Bigrams represent a meaningful combination of two words}, syntactic patterns and other global features are used as strong classification features for the supervised methods \cite{27}. The fact that the later approach leads in terms of accuracy and performance does make it a favorable choice to use for discovering different solutions to NLP problems similar to \ac{ned} and \ac{wsd}. However, due to the data scarcity problem, relying on large-amount of training data for different domains, tasks and languages cannot be seen as a realistic assumption and therefore significant manual effort is required \cite{30}. According to Sanderson \cite{sanderson1994}, improvements in the performance of information retrieval systems would be observed only if problems such as \ac{ned} and \ac{wsd} would perform at a level of at least 90\% accuracy \cite{sanderson1994,30}. The only possible way of achieving high levels of performance on these kind of tasks is by training supervised algorithms with large amounts of training data. Investigating on techniques and methodologies that would assure the generation of large-scale training data while keeping costs at minimal levels and still maintaining quality is the focus of the upcoming chapters. Before proceeding to the next section, understanding what the term "context" refers to in our particular problem scope is the topic of the next subsection.

